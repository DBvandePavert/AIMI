{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep image prior\n",
    "Baseline for superresolution/inpainting challenge, testing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from data import get_loaders\n",
    "from utils import *\n",
    "from network import skip\n",
    "torch.nn.Module.add = add_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path_train\": \"data/data_train_plus_test_sourceres/train/\",\n",
    "    \"path_test\": \"data/data_train_plus_test_sourceres/\",\n",
    "    \"N\": 1,\n",
    "    'verbose': 1,\n",
    "    \"batchsize\": 1\n",
    "}\n",
    "\n",
    "config = yaml.safe_load(open(\"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/configs/dip.yaml\"))\n",
    "\n",
    "if os.getcwd() != \"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/code\":\n",
    "    os.chdir(\"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/code\")\n",
    "\n",
    "# train_loader, val_loader = get_loaders(config)\n",
    "# set = train_loader.dataset.__getitem__(150)\n",
    "source = set[\"source\"][0]\n",
    "target = set[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "source = torch.load(\"source.pt\") # For Colab\n",
    "target = torch.load(\"target.pt\") # For Colab\n",
    "\n",
    "# Create masked image\n",
    "# source = setData[\"source\"][0] # For dataloader\n",
    "masked = np.zeros((256,192), dtype = float)\n",
    "masked[::,1::2] = source / 255\n",
    "masked = np.expand_dims(masked, axis=0)\n",
    "\n",
    "# Create mask\n",
    "mask = np.zeros((256,192), dtype = float)\n",
    "ones = np.ones((256,96), dtype = float)\n",
    "mask[::,1::2] = ones\n",
    "mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "# Show mask and masked\n",
    "plt.imshow(mask[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(masked[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Create tensors\n",
    "masked_tensor = torch.tensor(masked).cuda() # With GPU\n",
    "mask_tensor = torch.tensor(mask).cuda() # With GPU\n",
    "masked_tensor = torch.tensor(masked) # With CPU\n",
    "mask_tensor = torch.tensor(mask) # With CPU\n",
    "target_tensor = torch.tensor(target) # With CPU\n",
    "print(target_tensor.unsqueeze(0).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_depth = 32\n",
    "img_shape = 1\n",
    "# dtype = torch.cuda.FloatTensor # With GPU\n",
    "dtype = torch.FloatTensor # With CPU\n",
    "iterations = 2000\n",
    "show_every = 1\n",
    "plot = True\n",
    "\n",
    "# Create model\n",
    "net = skip(input_depth, img_shape, \n",
    "    num_channels_down = [128] * 5,\n",
    "    num_channels_up =   [128] * 5,\n",
    "    num_channels_skip =    [128] * 5,  \n",
    "    filter_size_up = 3, filter_size_down = 3, \n",
    "    upsample_mode='bilinear', filter_skip_size=1,\n",
    "    need_sigmoid=True, need_bias=True, pad='reflection', act_fun='LeakyReLU'\n",
    ").type(dtype)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.01)\n",
    "\n",
    "# Create initial input\n",
    "net_input = (0.1) * torch.rand((1,32,256,192))\n",
    "\n",
    "# Define loss and tensor types\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "# lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg').type(dtype)\n",
    "masked_tensor = masked_tensor.type(dtype)\n",
    "mask_tensor = mask_tensor.type(dtype)\n",
    "\n",
    "# Images list for gif\n",
    "images = []\n",
    "\n",
    "# losses list for plot\n",
    "mse_losses = []\n",
    "lpips_losses = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Init\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = net(net_input)\n",
    "    out = out.squeeze(0)\n",
    "\n",
    "    # Calculate losses\n",
    "    mse_loss =  mse(out * mask_tensor, masked_tensor)\n",
    "    # lpips_loss = lpips(out * mask_tensor, masked_tensor)\n",
    "\n",
    "    # Append losses\n",
    "    mse_losses.append(mse_loss.item())\n",
    "    # lpips_losses.append(lpips_loss.item())\n",
    "\n",
    "    # Set weights\n",
    "    mse_loss.backward()\n",
    "\n",
    "    # Regularization\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plot output and print losses\n",
    "    if plot and iteration % show_every == 0:\n",
    "        plt.imshow(out.cpu().permute(1,2,0).detach().numpy()[:,:,0] * 255, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print (f\"Iteration {iteration}\")\n",
    "        print(f\"MSE Loss {mse_loss.item()}\")\n",
    "        # print(f\"LPIPS Loss {lpips_loss.item()}\")\n",
    "\n",
    "    net_input = net_input + (1 / (30)) * torch.randn_like(net_input)\n",
    "\n",
    "plt.imshow(out.cpu().permute(1,2,0).detach().numpy()[:,:,0] * 255, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('med-img')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "235565d7d01cafd50be0b14e683116acf35705a4b38a399b849f52225f2dd45d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
