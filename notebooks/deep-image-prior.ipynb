{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep image prior\n",
    "Baseline for superresolution/inpainting challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from data import get_loaders\n",
    "from utils import *\n",
    "from network import skip\n",
    "torch.nn.Module.add = add_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path_train\": \"data/data_train_plus_test_sourceres/train/\",\n",
    "    \"path_test\": \"data/data_train_plus_test_sourceres/\",\n",
    "    \"N\": 1,\n",
    "    'verbose': 1,\n",
    "    \"batchsize\": 1\n",
    "}\n",
    "\n",
    "config = yaml.safe_load(open(\"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/configs/dip.yaml\"))\n",
    "\n",
    "if os.getcwd() != \"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/code\":\n",
    "    os.chdir(\"c:/Users/MauriceKingma/Documents/GitHub_repositories/AIMI/code\")\n",
    "\n",
    "# train_loader, val_loader = get_loaders(config)\n",
    "# set = train_loader.dataset.__getitem__(150)\n",
    "source = set[\"source\"][0]\n",
    "target = set[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mask and masked image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "# source = torch.load(\"source.pt\") # For Colab\n",
    "# target = torch.load(\"target.pt\") # For Colab\n",
    "\n",
    "# Create masked image\n",
    "# source = setData[\"source\"][0] # For dataloader\n",
    "masked = np.zeros((256,192), dtype = float)\n",
    "masked[::,1::2] = source / 255\n",
    "masked = np.expand_dims(masked, axis=0)\n",
    "\n",
    "# Create mask\n",
    "mask = np.zeros((256,192), dtype = float)\n",
    "ones = np.ones((256,96), dtype = float)\n",
    "mask[::,1::2] = ones\n",
    "mask = np.expand_dims(mask, axis=0)\n",
    "\n",
    "# Show mask and masked\n",
    "plt.imshow(mask[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(masked[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Create tensors\n",
    "# masked_tensor = torch.tensor(masked).cuda() # With GPU\n",
    "# mask_tensor = torch.tensor(mask).cuda() # With GPU\n",
    "masked_tensor = torch.tensor(masked) # With CPU\n",
    "mask_tensor = torch.tensor(mask) # With CPU\n",
    "target_tensor = torch.tensor(target) # With CPU\n",
    "print(target_tensor.unsqueeze(0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official deep-image-prior implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Figure.savefig() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Plot output and print losses\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m show_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# plt.imsave(\"test.png\", out.cpu().permute(1,2,0).detach().numpy()[:,:,0] * 255, cmap=\"gray\")\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, (out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[:,:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m))\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MauriceKingma\\anaconda3\\envs\\med-img\\lib\\site-packages\\matplotlib\\pyplot.py:979\u001b[0m, in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[39m.\u001b[39msavefig)\n\u001b[0;32m    977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msavefig\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    978\u001b[0m     fig \u001b[39m=\u001b[39m gcf()\n\u001b[1;32m--> 979\u001b[0m     res \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39msavefig(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m     fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mdraw_idle()   \u001b[39m# need this if 'transparent=True' to reset colors\u001b[39;00m\n\u001b[0;32m    981\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "\u001b[1;31mTypeError\u001b[0m: Figure.savefig() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Args\n",
    "# NET_TYPE = 'skip_depth6'\n",
    "# INPUT = 'noise'\n",
    "# input_depth = 32\n",
    "# LR = 0.01 \n",
    "# num_iter = 2\n",
    "# show_every = 10\n",
    "# figsize = 5\n",
    "# reg_noise_std = 0.03\n",
    "# OPT_OVER = 'net'\n",
    "# OPTIMIZER = 'adam'\n",
    "# PLOT = True\n",
    "# # dtype = torch.cuda.FloatTensor # With GPU\n",
    "# dtype = torch.FloatTensor # With CPU\n",
    "\n",
    "# Parameters\n",
    "input_depth = 32\n",
    "img_shape = 1\n",
    "# dtype = torch.cuda.FloatTensor # With GPU\n",
    "dtype = torch.FloatTensor # With CPU\n",
    "iterations = 2000\n",
    "show_every = 1\n",
    "plot = True\n",
    "\n",
    "# Create model\n",
    "net = skip(input_depth, img_shape, \n",
    "    num_channels_down = [128] * 5,\n",
    "    num_channels_up =   [128] * 5,\n",
    "    num_channels_skip =    [128] * 5,  \n",
    "    filter_size_up = 3, filter_size_down = 3, \n",
    "    upsample_mode='bilinear', filter_skip_size=1,\n",
    "    need_sigmoid=True, need_bias=True, pad='reflection', act_fun='LeakyReLU'\n",
    ").type(dtype)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.01)\n",
    "\n",
    "# Create initial input\n",
    "net_input = (0.1) * torch.rand((1,32,256,192))\n",
    "\n",
    "# Define loss and tensor types\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "# lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg').type(dtype)\n",
    "masked_tensor = masked_tensor.type(dtype)\n",
    "mask_tensor = mask_tensor.type(dtype)\n",
    "\n",
    "# Images list for gif\n",
    "images = []\n",
    "\n",
    "# losses list for plot\n",
    "mse_losses = []\n",
    "lpips_losses = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Init\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = net(net_input)\n",
    "    out = out.squeeze(0)\n",
    "\n",
    "    # Calculate losses\n",
    "    mse_loss =  mse(out * mask_tensor, masked_tensor)\n",
    "    # lpips_loss = lpips(out * mask_tensor, masked_tensor)\n",
    "\n",
    "    # Append losses\n",
    "    mse_losses.append(mse_loss.item())\n",
    "    # lpips_losses.append(lpips_loss.item())\n",
    "\n",
    "    # Set weights\n",
    "    mse_loss.backward()\n",
    "\n",
    "    # Regularization\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plot output and print losses\n",
    "    if plot and iteration % show_every == 0:\n",
    "        plt.imshow(out.cpu().permute(1,2,0).detach().numpy()[:,:,0] * 255, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print (f\"Iteration {iteration}\")\n",
    "        print(f\"MSE Loss {mse_loss.item()}\")\n",
    "        # print(f\"LPIPS Loss {lpips_loss.item()}\")\n",
    "\n",
    "    net_input = net_input + (1 / (30)) * torch.randn_like(net_input)\n",
    "\n",
    "plt.imshow(out.cpu().permute(1,2,0).detach().numpy()[:,:,0] * 255, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('med-img')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "235565d7d01cafd50be0b14e683116acf35705a4b38a399b849f52225f2dd45d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
